{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON3 = sys.version_info[0] == 3\n",
    "if PYTHON3:\n",
    "    from urllib.parse import urlencode\n",
    "    from urllib.request import urlopen\n",
    "    from urllib.error import HTTPError\n",
    "else:\n",
    "    from urllib import urlencode\n",
    "    from urllib2 import HTTPError, urlopen\n",
    "OAI = \"{http://www.openarchives.org/OAI/2.0/}\"\n",
    "ARXIV = \"{http://arxiv.org/OAI/arXiv/}\"\n",
    "BASE = 'http://export.arxiv.org/oai2?verb=ListRecords&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record(object):\n",
    "    \"\"\"\n",
    "    A class to hold a single record from ArXiv\n",
    "    Each records contains the following properties:\n",
    "    object should be of xml.etree.ElementTree.Element.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, xml_record):\n",
    "        \"\"\"if not isinstance(object,ET.Element):\n",
    "        raise TypeError(\"\")\"\"\"\n",
    "        self.xml = xml_record\n",
    "        self.id = self._get_text(ARXIV, 'id')\n",
    "        self.url = 'https://arxiv.org/abs/' + self.id\n",
    "        self.title = self._get_text(ARXIV, 'title')\n",
    "        self.abstract = self._get_text(ARXIV, 'abstract')\n",
    "        self.cats = self._get_text(ARXIV, 'categories')\n",
    "        self.created = self._get_text(ARXIV, 'created')\n",
    "        self.updated = self._get_text(ARXIV, 'updated')\n",
    "        self.doi = self._get_text(ARXIV, 'doi')\n",
    "        self.authors = self._get_authors()\n",
    "        self.affiliation = self._get_affiliation()\n",
    "\n",
    "    def _get_text(self, namespace, tag):\n",
    "        \"\"\"Extracts text from an xml field\"\"\"\n",
    "        try:\n",
    "            return self.xml.find(namespace + tag).text.strip().lower().replace('\\n', ' ')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def _get_authors(self):\n",
    "        authors_xml = self.xml.findall(ARXIV + 'authors/' + ARXIV + 'author')\n",
    "        last_names = [author.find(ARXIV + 'keyname') for author in authors_xml]\n",
    "        first_names = [author.find(ARXIV + 'forenames') for author in authors_xml]\n",
    "        full_names = [a+' '+b for a,b in zip(first_names, last_names)]\n",
    "        return full_names\n",
    "\n",
    "    def _get_affiliation(self):\n",
    "        authors = self.xml.findall(ARXIV + 'authors/' + ARXIV + 'author')\n",
    "        try:\n",
    "            affiliation = [author.find(ARXIV + 'affiliation') for author in authors]\n",
    "            return affiliation\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def output(self):\n",
    "        d = {\n",
    "            'title': self.title,\n",
    "            'id': self.id,\n",
    "            'abstract': self.abstract,\n",
    "            'categories': self.cats,\n",
    "            'doi': self.doi,\n",
    "            'created': self.created,\n",
    "            'updated': self.updated,\n",
    "            'authors': self.authors,\n",
    "            'affiliation': self.affiliation,\n",
    "            'url': self.url\n",
    "             }\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper(object):\n",
    "    \"\"\"\n",
    "    A class to hold info about attributes of scraping,\n",
    "    such as date range, categories, and number of returned\n",
    "    records. If `from` is not provided, the first day of\n",
    "    the current month will be used. If `until` is not provided,\n",
    "    the current day will be used.\n",
    "    Paramters\n",
    "    ---------\n",
    "    category: str\n",
    "        The category of scraped records\n",
    "    data_from: str\n",
    "        starting date in format 'YYYY-MM-DD'. Updated eprints are included even if\n",
    "        they were created outside of the given date range. Default: first day of current month.\n",
    "    date_until: str\n",
    "        final date in format 'YYYY-MM-DD'. Updated eprints are included even if\n",
    "        they were created outside of the given date range. Default: today.\n",
    "    t: int\n",
    "        Waiting time between subsequent calls to API, triggred by Error 503.\n",
    "    timeout: int\n",
    "        Timeout in seconds after which the scraping stops. Default: 300s\n",
    "    filter: dictionary\n",
    "        A dictionary where keys are used to limit the saved results. Possible keys:\n",
    "        subcats, author, title, abstract. See the example, below.\n",
    "    Example:\n",
    "    Returning all eprints from\n",
    "    ```\n",
    "        import arxivscraper.arxivscraper as ax\n",
    "        scraper = ax.Scraper(category='stat',date_from='2017-12-23',date_until='2017-12-25',t=10,\n",
    "                 filters={'affiliation':['facebook'],'abstract':['learning']})\n",
    "        output = scraper.scrape()\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, category, date_from=None, date_until=None, t=30, timeout=300, filters={}):\n",
    "        self.cat = str(category)\n",
    "        self.t = t\n",
    "        self.timeout = timeout\n",
    "        DateToday = datetime.date.today()\n",
    "        if date_from is None:\n",
    "            self.f = str(DateToday.replace(day=1))\n",
    "        else:\n",
    "            self.f = date_from\n",
    "        if date_until is None:\n",
    "            self.u = str(DateToday)\n",
    "        else:\n",
    "            self.u = date_until\n",
    "        self.url = BASE + 'from=' + self.f + '&until=' + self.u + '&metadataPrefix=arXiv&set=%s' % self.cat\n",
    "        self.filters = filters\n",
    "        if not self.filters:\n",
    "            self.append_all = True\n",
    "        else:\n",
    "            self.append_all = False\n",
    "            self.keys = filters.keys()\n",
    "\n",
    "    def scrape(self):\n",
    "        t0 = time.time()\n",
    "        tx = time.time()\n",
    "        elapsed = 0.0\n",
    "        url = self.url\n",
    "        print(url)\n",
    "        ds = []\n",
    "        k = 1\n",
    "        while True:\n",
    "\n",
    "            print('    fetching up to ', 1000 * k, 'records...')\n",
    "            try:\n",
    "                response = urlopen(url)\n",
    "            except HTTPError as e:\n",
    "                if e.code == 503:\n",
    "                    to = int(e.hdrs.get('retry-after', 30))\n",
    "                    print('Got 503. Retrying after {0:d} seconds.'.format(self.t))\n",
    "                    time.sleep(self.t)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "            k += 1\n",
    "            xml = response.read()\n",
    "            root = ET.fromstring(xml)\n",
    "            records = root.findall(OAI + 'ListRecords/' + OAI + 'record')\n",
    "            for record in records:\n",
    "                meta = record.find(OAI + 'metadata').find(ARXIV + 'arXiv')\n",
    "                record = Record(meta).output()\n",
    "                if self.append_all:\n",
    "                    ds.append(record)\n",
    "                else:\n",
    "                    save_record = False\n",
    "                    for key in self.keys:\n",
    "                        for word in self.filters[key]:\n",
    "                            if word.lower() in record[key]:\n",
    "                                save_record = True\n",
    "\n",
    "                    if save_record:\n",
    "                        ds.append(record)\n",
    "\n",
    "            try:\n",
    "                token = root.find(OAI + 'ListRecords').find(OAI + 'resumptionToken')\n",
    "            except:\n",
    "                return 1\n",
    "            if token is None or token.text is None:\n",
    "                break\n",
    "            else:\n",
    "                url = BASE + 'resumptionToken=%s' % token.text\n",
    "\n",
    "            ty = time.time()\n",
    "            elapsed += (ty-tx)\n",
    "            if elapsed >= self.timeout:\n",
    "                break\n",
    "            else:\n",
    "                tx = time.time()\n",
    "\n",
    "        t1 = time.time()\n",
    "        print('fetching is completed in {0:.1f} seconds.'.format(t1 - t0))\n",
    "        print ('Total number of records {:d}'.format(len(ds)))\n",
    "        return ds\n",
    "\n",
    "\n",
    "def search_all(df, col, *words):\n",
    "    \"\"\"\n",
    "    Return a sub-DataFrame of those rows whose Name column match all the words.\n",
    "    source: https://stackoverflow.com/a/22624079/3349443\n",
    "    \"\"\"\n",
    "    return df[np.logical_and.reduce([df[col].str.contains(word) for word in words])]\n",
    "\n",
    "cats = [ 'cs']\n",
    "subcats = {'cs': ['cs.AI', 'cs.CV', 'cs.DS',\n",
    "              'cs.DB', 'cs.HC', 'cs.LG', 'cs.MA',\n",
    "              'cs.NE', 'cs.CL',  'cs.CC','cs.RO']\n",
    "          }\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper.arxivscraper as ax\n",
    "\n",
    "sc = ax.Scraper(category='cs',date_from='2019-11-20',date_until='2019-12-20',t=10,\n",
    "                 filters={'abstract':['learning']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/oai2?verb=ListRecords&from=2019-11-20&until=2019-12-20&metadataPrefix=arXiv&set=cs\n",
      "fetching up to  1000 records...\n",
      "fetching up to  2000 records...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8e320d8883aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arxivscraper/arxivscraper.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOAI\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'arXiv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arxivscraper/arxivscraper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xml_record)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'updated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_authors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffiliation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_affiliation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arxivscraper/arxivscraper.py\u001b[0m in \u001b[0;36m_get_authors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mauthors_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'authors/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mlast_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'keyname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors_xml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mfirst_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'forenames'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors_xml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mfull_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfull_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arxivscraper/arxivscraper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mauthors_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'authors/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mlast_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'keyname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors_xml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mfirst_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARXIV\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'forenames'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors_xml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mfull_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfull_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "ds = sc.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
